{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Introduction to Pytorch Lightning",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics"
   ],
   "metadata": {
    "id": "gK84XoasrgAC",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.713636Z",
     "start_time": "2024-11-05T18:52:27.402934Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu-IgiEnanUm"
   },
   "source": [
    "# Pytorch-Lightning : Training made Easier\n",
    "\n",
    "Time : 4 hours\n",
    "\n",
    "In the Tutorial session, we used PyTorch to train different models for Binary Classification. In the tutorial, few things were done :\n",
    "\n",
    "\n",
    "*   We created a Training/Testing Loop and trained our models\n",
    "*   We created a Trainer Class to gather all loops to perform the Training/Testing.\n",
    "\n",
    "\n",
    "As you have seen, writing the training and testing loop can quickly be indigest. One can get easily lost.\n",
    "\n",
    "Let us introduce you Pytorch Lightning\n",
    "\n",
    "<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a88de56e65d2ea6bc203ce178a1cecbe9b50a0ac/68747470733a2f2f6769746875622e636f6d2f5079546f7263684c696768746e696e672f7079746f7263682d6c696768746e696e672f7261772f312e342e392f646f63732f736f757263652f5f7374617469632f696d616765732f6c6f676f2e706e67\">\n",
    "\n",
    "\n",
    "Pytorch lightning will handle a lot of things for you. It creates a Trainer which is a Code Management trick used by many companies (Meta, Google..) in order to get much more digest code.\n",
    "\n",
    "\n",
    "More Information on : https://www.pytorchlightning.ai/\n",
    "\n",
    "Goal of this lab :\n",
    "\n",
    "* Use Pytorch Lightning for Training\n",
    "* Learn to use Pytorch-Lightning\n",
    "* Do classification on MNIST, CIFAR-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_cnXBS7boC7"
   },
   "source": [
    "# I - Classify Numbers using Lightning\n",
    "\n",
    "In this part, we are interested in classifying digit images ranging from 0 to 9. \n",
    "We will use the Lightning framework for code management. What's interesting about Lightning is that you can plug in your Torch modules without any modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xBCfzUJcJO9"
   },
   "source": [
    "## a - LightningDataModule : MNIST\n",
    "\n",
    "As you have seen in the Tutorial, you need to create your Dataset Class.\n",
    "\n",
    "As a reminder :    \n",
    " The Dataset class returns one sample of your dataset at a time. The main methods of the Dataset class are \n",
    "\n",
    "*   __getitem__ : which fetched a sample at a given index\n",
    "*   __len__ : which returns the len of the total dataset\n",
    "\n",
    "The Dataset is loaded into a DataLoader. That Dataloader is then used to **fetch and send data as batches** for your Model.\n",
    "\n",
    "You will see that using Lightning makes things clearer. LightningDataModule allows you to write cleaner Code and fit easily your data to your model.\n",
    "\n",
    "You can always, use the basic Pytorch Dataloader in a separate code."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are first going to work with the MNIST dataset. There is already a MNIST class provided in the Torchvision library, so we don't have to code the Dataset implementation ourself.\n",
    "* Fill in the blanks"
   ],
   "metadata": {
    "id": "xaDUVAFvqp5h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploratory Data Analysis : Discovering the Data\n",
    "\n",
    "Before tackling the classification task it is essential to explore the data we are working on, and understand its specifics.\n",
    "Perform an Exploratory Data Analysis (EDA) on the MNIST Dataset :\n",
    "\n",
    "1.   What type of Data do you have ? (Images, Texts, Sound..)\n",
    "2.   How many Data do you have ? \n",
    "3.   What's in a sample (1 element of the Dataset)\n",
    "4.   Is the Dataset umbalanced ?\n",
    "5.   What's the shape of any input sample ?\n",
    "6.   ....\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "qrxvDv_8r9mU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the Training Split of MNIST Dataset\n",
    "dataset_train  = MNIST('', train=True, download=True)\n",
    "dataset_test = MNIST('', train=False, download=True)"
   ],
   "metadata": {
    "id": "EDrwvhJAsEz_",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.760934Z",
     "start_time": "2024-11-05T18:52:29.714613Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : What's the length of the train and test split ?\n",
    "\n",
    "print(\"the length if the test dataset is :\",len(dataset_test))\n",
    "print(\"the length of the train dataset is :\",len(dataset_train))"
   ],
   "metadata": {
    "id": "8vwYTFrhtZ43",
    "outputId": "f3074ec1-6863-4aa4-aac6-540ae2ade880",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.765084Z",
     "start_time": "2024-11-05T18:52:29.762181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length if the test dataset is : 10000\n",
      "the length of the train dataset is : 60000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Retrieve one sample of the Dataset.\n",
    "sample, label = dataset_train[0]\n",
    "\n",
    "# TODO : What is in a sample ? Print the sample to understand\n",
    "print(f\"Sample shape: {sample}, Label: {label}\")"
   ],
   "metadata": {
    "id": "IF-0t0ETt1Fa",
    "outputId": "d8fbc436-4601-4d53-cee5-e046526f8522",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.772075Z",
     "start_time": "2024-11-05T18:52:29.767203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: <PIL.Image.Image image mode=L size=28x28 at 0x107659D60>, Label: 5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Plot the image in the sample. Does it correspond to the second element of the sample ?\n",
    "\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "pNqZ8DmhuDU0",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.861417Z",
     "start_time": "2024-11-05T18:52:29.772875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : What's the shape of the input image.\n",
    "shape = sample.size\n",
    "\n",
    "print(f\"The shape of the sample is :{shape}\")"
   ],
   "metadata": {
    "id": "QUYEXO0TuZ24",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.866772Z",
     "start_time": "2024-11-05T18:52:29.863173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the sample is :(28, 28)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBDDrYrKlr8X"
   },
   "source": [
    "### Lightning DataModule : Dataset and DataLoader Embedded \n",
    "\n",
    "Pytorch Lightning introduces a new way to define and organize our dataset via \"data module\". They neatly encompass our training, validation and testing datasets and provide their dataloaders as well.\n",
    "\n",
    "Have a look at : https://pytorch.org/vision/stable/datasets.html\n",
    "\n",
    "We are now going to define a `LightningDataModule` for the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HsUhr7BihqxX",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.874499Z",
     "start_time": "2024-11-05T18:52:29.868816Z"
    }
   },
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        self.data_dir = ''\n",
    "        self.batch_size_train, self.batch_size_valid, self.batch_size_test = 32,32,32\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # This method is used to download beforehand the dataset if needed.\n",
    "        # TODO : Load the train and test dataset.\n",
    "        dataset_train = MNIST(self.data_dir, train=True, download=True)\n",
    "        dataset_test = MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage):\n",
    "        # We need to setup our module. We have \n",
    "        #  1. A training set that we will **fit** our model to\n",
    "        #  2. A testing set used to **test** our models prediction.\n",
    "        \n",
    "        # The stage variable corresponds to those two steps : \n",
    "        # stage in {fit, test, None}\n",
    "\n",
    "        # First stage is 'fit' (or None)\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # We create a validation split to watch the training.\n",
    "\n",
    "            # Which dataset do we load for training ?\n",
    "            mnist_dataset = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            train_size = int(0.8 * len(mnist_dataset))\n",
    "            valid_size = len(mnist_dataset) - train_size\n",
    "            mnist_train, mnist_valid =  torch.utils.data.random_split(mnist_dataset, [train_size, valid_size]) #random split into valid and train set\n",
    "            \n",
    "            # Load the datasets as attributes of the Module. Don't forget you validation split\n",
    "            self.mnist_train, self.mnist_valid = mnist_train, mnist_valid\n",
    "\n",
    "        # Second stage is 'test' \n",
    "        if stage == \"test\" or stage is None:\n",
    "\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "            # Question : What additional set can we create ? Why ?\n",
    "            # We can do the same and split the test set into validation and test\n",
    "            # It is not a usual application\n",
    "            # It can be done for hyperparameters application\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # TODO : Now create your Training DataLoader\n",
    "        return torch.utils.data.DataLoader(self.mnist_train, batch_size=self.batch_size_train, shuffle=True)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # TODO : Now create your Validation DataLoader\n",
    "        return torch.utils.data.DataLoader(self.mnist_valid, batch_size=self.batch_size_valid)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # TODO : Now create your Testing DataLoader\n",
    "        return torch.utils.data.DataLoader(self.mnist_test, batch_size=self.batch_size_test)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71GzlDYqj5xY"
   },
   "source": [
    "## b - LightningModule :  MNIST Classifier \n",
    "\n",
    "Similarly to `LightningDataModule`, pytorch Lightning provides a `LightningModule` that gathers every functions needed for the training and testing of our pytorch model.\n",
    " \n",
    "Design a model to perform Classification using the `LightningModule` class layout. Again, ask yourself the following questions: \n",
    "* What task is it ?\n",
    "* What data do I have ?\n",
    "* What learning rate should I use ?\n",
    "* What could be my loss ? Why ?\n",
    "* What non-linearity should I use ?\n",
    "* How do I evaluate my model ? (TorchMetrics is your friend)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUBuWwX3iypq",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:52:29.884019Z",
     "start_time": "2024-11-05T18:52:29.876104Z"
    }
   },
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, output_shape, learning_rate=1e-3):\n",
    "        super(MNISTClassifier,self).__init__()\n",
    "        # what is the output_shape of your model ?\n",
    "        self.output_shape = output_shape\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate  # Save the learning rate as an instance attribute\n",
    "\n",
    "        # TODO : Define your model here, be careful, your model will be an instance of the class. Watch  out for the input data.\n",
    "\n",
    "        # Ddefine the model (réseau de neurones simple pour MNIST)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), #Flatten images 28x28 into a vector of 784\n",
    "            nn.Linear(28 * 28, 128),  # First fully connected layer\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            nn.Linear(128, 64),  # Second fully connected layer\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            nn.Linear(64, self.output_shape)  # Final layer for the 10 classe output\n",
    "        )\n",
    "\n",
    "        # Accuracy using torchmetrics\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.output_shape)\n",
    "\n",
    "        #\n",
    "        self.train_acc = self.accuracy\n",
    "        self.valid_acc = self.accuracy\n",
    "        self.test_acc = self.accuracy\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO : What would be the forward steps of this classifier ? Return the output of our classifier given an input batch x.\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO : Choose your optimizer : https://pytorch.org/docs/stable/optim.html\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # TODO : Define your Training Step\n",
    "        # Extract data from the batch (x = images, y = labels)\n",
    "        x, y = batch\n",
    "    \n",
    "        # Pass the images through the model\n",
    "        logits = self(x) # The model returns raw logits (unnormalized outputs)\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss (suitable for multi-class classification)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "\n",
    "        # Calculate accuracy using the TorchMetrics Accuracy method defined in __init__\n",
    "        acc = self.train_acc(logits.softmax(-1), y)\n",
    "\n",
    "        # Log the training accuracy (on_epoch=True means we log it after the full epoch, not after each batch)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)  # Log training accuracy\n",
    "\n",
    "        # Log the training loss (logged for every batch)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Return the loss, which will be used for backpropagation\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # TODO : Define your Validation Step\n",
    "        # What is the difference between the Training and the Validation Step ?\n",
    "        # there is no backpropagation on this step, the only aim is to measure the performance of the model, no weights update\n",
    "\n",
    "        # Extract data from the batch (x = images, y = labels)\n",
    "        x, y = batch\n",
    "\n",
    "        # Pass the images through the model (forward pass)\n",
    "        logits = self(x)  # The model returns raw logits (unnormalized outputs)\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss (suitable for multi-class classification)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "\n",
    "        # Calculate accuracy using TorchMetrics Accuracy method defined in __init__\n",
    "        acc = self.test_acc(logits.softmax(-1), y)\n",
    "\n",
    "        # Log the validation accuracy and loss\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True)  # Log validation accuracy\n",
    "        self.log('val_loss', loss)  # Log validation loss\n",
    "\n",
    "        # Return the loss (even though it's not used for backpropagation in validation)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # TODO : Define your Test Step\n",
    "        # Extract data from the batch (x = images, y = labels)\n",
    "        x, y = batch\n",
    "\n",
    "        # Difference ?\n",
    "        #Again no back propagation\n",
    "        # Pass the images through the model (forward pass)\n",
    "        logits = self(x)  # The model returns raw logits (unnormalized outputs)\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss (suitable for multi-class classification)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "\n",
    "        # Calculate accuracy using TorchMetrics Accuracy method defined in __init__\n",
    "        acc = self.test_acc(logits.softmax(-1), y)\n",
    "\n",
    "\n",
    "        # Log the test loss and accuracy\n",
    "        self.log('test_loss', loss)  # Log test loss\n",
    "        self.log('test_acc', acc)    # Log test accuracy\n",
    "\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    def test_epoch_start(self):\n",
    "        # Initialize accuracy accumulator\n",
    "        self.acc = 0  # This will store the sum of accuracies across batches\n",
    "        self.num_batches = 0  # Counter for the number of batches\n",
    "\"\"\"\n",
    "    def on_test_epoch_end(self):\n",
    "        # Compute the final test accuracy\n",
    "        final_test_acc = self.test_acc.compute()\n",
    "\n",
    "        # Log the final test accuracy\n",
    "        self.log('Final Test Accuracy', final_test_acc)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TV-SLU6kWV7"
   },
   "source": [
    "## c - Did you say Train ?\n",
    "\n",
    "Let's train the model. \n",
    "\n",
    "We create our so called Trainer that will handle a lot of thing for us. Lightning trainer is full of interesting assets that helps you for your training. The lightning trainer is a much more evolved Trainer than the one in the Tutorial.\n",
    "\n",
    "To get a glance of what Lightning Trainer can give :\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html\n",
    "\n",
    "It also easily lets us using TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L9wyr4pjj5R-",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:54:32.413276Z",
     "start_time": "2024-11-05T18:52:29.886665Z"
    }
   },
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"introduction to Lightning\") #to save on a file\n",
    "\n",
    "dm = MNISTDataModule()\n",
    "model = MNISTClassifier(10)\n",
    "\n",
    "trainer = pl.Trainer(devices=\"auto\",max_epochs=10,accelerator='gpu',logger=tb_logger)\n",
    "trainer.fit(model, dm) #the training part\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model    | Sequential         | 109 K  | train\n",
      "1 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94f8682f32804294b5a6e511e9f9c17a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaheergoulam/PycharmProjects/Deep_Learning/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/jaheergoulam/PycharmProjects/Deep_Learning/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d13e0c7422343b69c536da5fe8e7a9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca381a1185354b6ea3f144aeab9e5c12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9192c4c20cf3499780cd19f320a95e2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b80e2d8dae3431bab27d52ffb57fa99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45b207d803924aa0884b66d85e7531fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bd167134c0f4c2bb6ffe26bcfa3ab53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "711d5c93b9c445fd8acdf207de56ab71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67dfe795b40742479bec057901602760"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a05bc065575f47ba9b1a538b06a9d309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0b4ce66e4a644fe80ecd192d2e346cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d11228189348445cbea112820b712920"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej56Kf9Zkefo"
   },
   "source": "Oh it's training ! Happy ? Easy ? Let's test the model"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaWmGYVTjRz5"
   },
   "source": [
    "## d - Did you say Test ?\n",
    "\n",
    "For testing, well it's pretty easy "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "idVeeYZKVvSS",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:59:19.294471Z",
     "start_time": "2024-11-05T18:59:17.674097Z"
    }
   },
   "source": "trainer.test(model,dm)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e0671290f13411193be0ea532fc7bbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   Final Test Accuracy      0.9751703143119812\n",
      "        test_acc            0.9724000096321106\n",
      "        test_loss           0.11991356313228607\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11991356313228607,\n",
       "  'test_acc': 0.9724000096321106,\n",
       "  'Final Test Accuracy': 0.9751703143119812}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO_fuzVPMNX5"
   },
   "source": [
    "## e - TensorBoard\n",
    "\n",
    "TensorBoard is a really useful tool. Indeed, it let's you register interesting values during training and plot them INTERACTIVELY. You might have seen a self.log line in the Validation and Training steps. \n",
    "The self.log saves the loss value into a TensorBoard readable file. We can also add images or other values using self.log\n",
    "\n",
    "In fact, look at the checkpoint created by the training. You might see 3 files :\n",
    "* Checkpoint\n",
    "* event.out....\n",
    "* hparam.yaml\n",
    "\n",
    "Let's open tensorboard to see how the training was. Tensorboard is loadable using magic_python commands.\n",
    "More info on TensorBoard : https://www.tensorflow.org/tensorboard/get_started\n",
    "\n",
    "Another popular alternative to Tensorboard, also usable with pytorch lightning, is \"Weight and Biases\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JzLzFpFdkdoM",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:54:34.436936Z",
     "start_time": "2024-11-05T18:54:34.421697Z"
    }
   },
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"/content/introduction to Lightning/default/version_0\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3306), started 0:23:11 ago. (Use '!kill 3306' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4d712c8625e09aec\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4d712c8625e09aec\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCx4XtQojqBf"
   },
   "source": [
    "Pytorch Lightning can be used along PyTorch. We encourage you to use PyTorch Lightning during your Lab Sessions and Career as it simplifies a lot of things for you (MultiGPU, Learning Rate Decay...)\n",
    "\n",
    "<img src='https://c.tenor.com/VyApQ-jWyV0AAAAC/happy-borat.gif'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# II - Classify Objects using Lightning \n",
    "\n",
    "We will now turn to another classification task, this time object classification. We use the CIFAR-10 dataset which is made of 10 different classes of objects which we aim to distinguish. This part of the lab will be less restricted and more free. You now should have a sense of how to use the Lightning Framework. \n",
    "Be creative.\n"
   ],
   "metadata": {
    "id": "rq2vIzXbyXtk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a - Baseline : Creating your own Model\n",
    "\n",
    "In this first section, we will create a Simple Model and perform all steps from part 1 with the needed changes.\n",
    "\n",
    "*   **What's your final accuracy ?**\n"
   ],
   "metadata": {
    "id": "WVi6aj84y97g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### i - DataModule"
   ],
   "metadata": {
    "id": "LodackDWzb9k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ],
   "metadata": {
    "id": "La8kvImxzgaf",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:54:34.546676Z",
     "start_time": "2024-11-05T18:54:34.437989Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mToTensor(),transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m), (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m))])\n\u001B[0;32m----> 2\u001B[0m trainset \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mCIFAR10(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m'\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m                                         download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m      4\u001B[0m testset \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mCIFAR10(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m'\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m      5\u001B[0m                                        download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : EDA "
   ],
   "metadata": {
    "id": "dUes-3TPFhW8",
    "ExecuteTime": {
     "end_time": "2024-11-05T18:54:34.549327Z",
     "start_time": "2024-11-05T18:54:34.549084Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Create your DataModule"
   ],
   "metadata": {
    "id": "-hJKuGjwFHxE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii - Module"
   ],
   "metadata": {
    "id": "2ykxzmaMzhEr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Create your Module"
   ],
   "metadata": {
    "id": "iZWj3Qp8zgej"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii - Train"
   ],
   "metadata": {
    "id": "SjDUQJBhzloB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Train"
   ],
   "metadata": {
    "id": "i1-7v8j9zoAJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iv - Test"
   ],
   "metadata": {
    "id": "I8RmPe1ozoxq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Test"
   ],
   "metadata": {
    "id": "zXDiuQOMzoxr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Does your model perform well on the CIFAR Dataset ?**\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Ls7EK0Az2YFr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b - The OG Model : Finetuning a Model\n",
    "\n",
    "If your model performed well on the CIFAR-10 Dataset, congrats. But let's achieve better results. For industrial works, we often pretrain a model on a large dataset (ImageNet or internal Dataset), and then fine-tune the model on the Dataset of interest.\n",
    "\n",
    "* **What's the intuition behind fine-tuning ?**"
   ],
   "metadata": {
    "id": "hfcW6jcizB1I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### i - Importing a Pretrained Model\n",
    "\n",
    "We will import a ConvNext model. Why ? It's said to be a really good backbone that competes with the Transformer models. Let's load the model. \n",
    "We are going to use TorchSummary to print what the size of the inputs and outputs are."
   ],
   "metadata": {
    "id": "0Qxg-fff7Ofw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torchvision\n",
    "model = torchvision.models.convnext_small(weights='DEFAULT')\n",
    "\n",
    "# TODO : Using torchsummary, print a summary of the model\n",
    "from torchsummary import summary\n",
    "summary(model)\n"
   ],
   "metadata": {
    "id": "DIgUs9Fd7FW7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Using torchsummary, send an image of the same size as a sample of CIFAR-10\n",
    "summary(model, ...) # ... = input shape as a tuple (C,H,W)\n"
   ],
   "metadata": {
    "id": "x29gjtvd94Jr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **What is the output size of the model ?**\n",
    "* **What will be the issue of using this model as is to perform classification on the CIFAR-10 Dataset ?**\n"
   ],
   "metadata": {
    "id": "vIpoh9qP-Nll"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : According to your answer to the previous questions, perform the changes.\n",
    "# You can access each layers using model.name_of_layer"
   ],
   "metadata": {
    "id": "3XBym9hsB2Z1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ii - DataModule"
   ],
   "metadata": {
    "id": "LcainrOT51rP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : EDA \n",
    "\n",
    "# TODO : Create your DataModule"
   ],
   "metadata": {
    "id": "x3WlWJvy51rP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iii - Module\n",
    "\n"
   ],
   "metadata": {
    "id": "3aUiAPtd51rP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Create your Module\n",
    "\n",
    "# Careful : How should your learning rate be ?"
   ],
   "metadata": {
    "id": "W0zUy7CJ51rP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### iv - Train"
   ],
   "metadata": {
    "id": "RwPYkPT551rQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Train"
   ],
   "metadata": {
    "id": "2GdvMUH-51rQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### v - Test"
   ],
   "metadata": {
    "id": "hEJLdQuZ51rQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO : Test"
   ],
   "metadata": {
    "id": "DvxjzGdV51rQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "* **What is your final accuracy ?**\n",
    "* **Is Fine Tuning a model better than creating your own model ?**"
   ],
   "metadata": {
    "id": "USu9oo2wCV2j"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
